{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db17b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\balgy\\Desktop\\job\\happycolor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:01,  5.15it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 45.45it/s]it/s]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 18.23it/s]\n",
      "100%|██████████| 28/28 [01:22<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from diffusers import FluxKontextPipeline, FluxTransformer2DModel, GGUFQuantizationConfig\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "ckpt_path = os.path.join(\"models\", \"flux1-kontext-dev-Q4_0.gguf\")\n",
    "# path_lora = os.path.join(\"models\", \"aidmaImageUpgrader-FLUX-V0.1.safetensors\")\n",
    "\n",
    "# Load transformer with GGUF quantization\n",
    "transformer = FluxTransformer2DModel.from_single_file(\n",
    "    ckpt_path,\n",
    "    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load FLUX Kontext pipeline\n",
    "pipe = FluxKontextPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-Kontext-dev\",\n",
    "    transformer=transformer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.transformer.config.in_channels = 64\n",
    "\n",
    "input_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\n",
    "\n",
    "images = pipe(\n",
    "            prompt='a girl',\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea500ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balgy\\Desktop\\job\\happycolor\\.venv\\Lib\\site-packages\\PIL\\Image.py:3524\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3521\u001b[39m     fp = io.BytesIO(fp.read())\n\u001b[32m   3522\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3524\u001b[39m prefix = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m(\u001b[32m16\u001b[39m)\n\u001b[32m   3526\u001b[39m preinit()\n\u001b[32m   3528\u001b[39m warning_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n",
      "\u001b[31mAttributeError\u001b[39m: 'Image' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "Image.open(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cfcf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\balgy\\Desktop\\job\\happycolor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 153.86it/s]t/s]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 28.18it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "PEFT backend is required for this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      7\u001b[39m transformer = FluxTransformer2DModel.from_single_file(\n\u001b[32m      8\u001b[39m             ckpt_path,\n\u001b[32m      9\u001b[39m             quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n\u001b[32m     10\u001b[39m             torch_dtype=torch.bfloat16,\n\u001b[32m     11\u001b[39m         )\n\u001b[32m     12\u001b[39m pipe = FluxPipeline.from_pretrained(\n\u001b[32m     13\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mblack-forest-labs/FLUX.1-dev\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m             transformer=transformer,\n\u001b[32m     15\u001b[39m             torch_dtype=torch.bfloat16,\n\u001b[32m     16\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_lora_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_lora\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m pipe.enable_model_cpu_offload()\n\u001b[32m     21\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mcute anime girl with massive fluffy fennec ears and a big fluffy tail blonde messy long hair blue eyes wearing a maid outfit with a long black gold leaf pattern dress and a white apron mouth open holding a fancy black forest cake with candles on top in the kitchen of an old dark Victorian mansion lit by candlelight with a bright window to the foggy forest and very expensive stuff everywhere\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balgy\\Desktop\\job\\happycolor\\.venv\\Lib\\site-packages\\diffusers\\loaders\\lora_pipeline.py:1657\u001b[39m, in \u001b[36mFluxLoraLoaderMixin.load_lora_weights\u001b[39m\u001b[34m(self, pretrained_model_name_or_path_or_dict, adapter_name, hotswap, **kwargs)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1631\u001b[39m \u001b[33;03mLoad LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and\u001b[39;00m\n\u001b[32m   1632\u001b[39m \u001b[33;03m`self.text_encoder`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1654\u001b[39m \u001b[33;03m        See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].\u001b[39;00m\n\u001b[32m   1655\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m USE_PEFT_BACKEND:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPEFT backend is required for this method.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1659\u001b[39m low_cpu_mem_usage = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mlow_cpu_mem_usage\u001b[39m\u001b[33m\"\u001b[39m, _LOW_CPU_MEM_USAGE_DEFAULT_LORA)\n\u001b[32m   1660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_peft_version(\u001b[33m\"\u001b[39m\u001b[33m>=\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0.13.1\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: PEFT backend is required for this method."
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import FluxPipeline, FluxTransformer2DModel, GGUFQuantizationConfig\n",
    "\n",
    "ckpt_path = os.path.join(\"models\", \"flux1-dev-Q4_0.gguf\")\n",
    "transformer = FluxTransformer2DModel.from_single_file(\n",
    "            ckpt_path,\n",
    "            quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "            \"black-forest-labs/FLUX.1-dev\",\n",
    "            transformer=transformer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "\n",
    "prompt = \"cute anime girl with massive fluffy fennec ears and a big fluffy tail blonde messy long hair blue eyes wearing a maid outfit with a long black gold leaf pattern dress and a white apron mouth open holding a fancy black forest cake with candles on top in the kitchen of an old dark Victorian mansion lit by candlelight with a bright window to the foggy forest and very expensive stuff everywhere\"\n",
    "image = pipe(\n",
    "    prompt, \n",
    ").images[0]\n",
    "image.save(\"./image.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4342d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
